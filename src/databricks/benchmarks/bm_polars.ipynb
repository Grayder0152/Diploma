{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33705650-4d5e-4f82-b47a-9757ad7316aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"polars[rt64]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ad6895-b6af-4421-8e3a-0f1789e8ad0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class PolarsBenchmark:\n",
    "    \"\"\"\n",
    "    Optimized Polars benchmark suite.\n",
    "    Handles CSV folders with inconsistent schemas (extra/missing columns).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tasks: List[str] | None = None):\n",
    "        tasks = tasks or [\"*\"]\n",
    "\n",
    "        self.tasks = {\n",
    "            name: getattr(self, name)\n",
    "            for name in dir(self)\n",
    "            if callable(getattr(self, name))\n",
    "            and not name.startswith(\"_\")\n",
    "            and (name in tasks or \"*\" in tasks)\n",
    "        }\n",
    "\n",
    "    def _lazy_from_path(self, path: str) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Reads CSV files from:\n",
    "        - a single file\n",
    "        - a folder\n",
    "        - nested subfolders\n",
    "        Returns a LazyFrame with schema-relaxed concatenation.\n",
    "        \"\"\"\n",
    "\n",
    "        p = Path(path)\n",
    "\n",
    "        if p.is_dir():\n",
    "            files = sorted(str(f) for f in p.rglob(\"*.csv\"))\n",
    "\n",
    "            if not files:\n",
    "                raise FileNotFoundError(f\"No CSV files found under {path}\")\n",
    "\n",
    "            lazy_frames = [\n",
    "                pl.scan_csv(f, low_memory=True)\n",
    "                for f in files\n",
    "            ]\n",
    "\n",
    "            return pl.concat(lazy_frames, how=\"diagonal_relaxed\")\n",
    "\n",
    "        if \"*\" in path:\n",
    "            files = sorted(glob.glob(path))\n",
    "            if not files:\n",
    "                raise FileNotFoundError(f\"No matching CSV for pattern {path}\")\n",
    "\n",
    "            lazy_frames = [pl.scan_csv(f, low_memory=True) for f in files]\n",
    "            return pl.concat(lazy_frames, how=\"diagonal_relaxed\")\n",
    "\n",
    "        if p.is_file() and p.suffix.lower() == \".csv\":\n",
    "            return pl.scan_csv(str(p), low_memory=True)\n",
    "\n",
    "        raise ValueError(f\"Path is neither file, folder, nor pattern: {path}\")\n",
    "\n",
    "    def _lazy_union(self, paths: List[str]) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        UNION all folders/files with safe schema merging.\n",
    "        \"\"\"\n",
    "        lfs = [self._lazy_from_path(p) for p in paths]\n",
    "        return pl.concat(lfs, how=\"diagonal_relaxed\")\n",
    "\n",
    "    def group_by_model(self, data_path: str) -> pl.DataFrame:\n",
    "        df = self._lazy_from_path(data_path)\n",
    "\n",
    "        return (\n",
    "            df.group_by(\"model\")\n",
    "            .agg([\n",
    "                pl.col(\"smart_5_raw\").mean().alias(\"avg_smart_5_raw\"),\n",
    "                pl.col(\"smart_5_raw\").std().alias(\"std_smart_5_raw\"),\n",
    "                pl.col(\"serial_number\").n_unique().alias(\"unique_disks\"),\n",
    "                pl.col(\"failure\").sum().alias(\"failures_total\"),\n",
    "            ])\n",
    "            .sort(\"avg_smart_5_raw\", descending=True)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "    def union_and_aggregate(self, data_folders: List[str]) -> pl.DataFrame:\n",
    "        if len(data_folders) < 2:\n",
    "            raise ValueError(\"At least two folders/files required\")\n",
    "\n",
    "        df = self._lazy_union(data_folders)\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"smart_5_raw\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"smart_187_raw\").cast(pl.Float64, strict=False),\n",
    "            pl.col(\"failure\").cast(pl.Int64, strict=False),\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            df.select([\n",
    "                pl.col(\"smart_5_raw\").mean().alias(\"mean_smart_5\"),\n",
    "                pl.col(\"smart_187_raw\").mean().alias(\"mean_smart_187\"),\n",
    "                pl.col(\"failure\").sum().alias(\"total_failures\"),\n",
    "                pl.len().alias(\"records_total\"),\n",
    "            ])\n",
    "            .with_columns(\n",
    "                (pl.col(\"total_failures\") / pl.col(\"records_total\")).alias(\"failure_rate\")\n",
    "            )\n",
    "            .collect(engine=\"streaming\")\n",
    "        )\n",
    "\n",
    "    def join_adjacent_days(self, data_folders: List[str]) -> pl.DataFrame:\n",
    "        if len(data_folders) < 2:\n",
    "            raise ValueError(\"At least two folders/files required\")\n",
    "\n",
    "        base = (\n",
    "            self._lazy_from_path(data_folders[0])\n",
    "            .select([\n",
    "                pl.col(\"serial_number\"),\n",
    "                pl.col(\"smart_5_raw\").cast(pl.Float64).alias(\"smart_5_raw_0\"),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        for i, p in enumerate(data_folders[1:], start=1):\n",
    "            nxt = (\n",
    "                self._lazy_from_path(p)\n",
    "                .select([\n",
    "                    pl.col(\"serial_number\"),\n",
    "                    pl.col(\"smart_5_raw\").cast(pl.Float64).alias(f\"smart_5_raw_{i}\"),\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            base = (\n",
    "                base.join(\n",
    "                    nxt,\n",
    "                    on=\"serial_number\",\n",
    "                    how=\"inner\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        delta_exprs = [\n",
    "            (pl.col(f\"smart_5_raw_{i}\") - pl.col(\"smart_5_raw_0\")).alias(f\"smart_delta_{i}\")\n",
    "            for i in range(1, len(data_folders))\n",
    "        ]\n",
    "\n",
    "        result = base.with_columns(delta_exprs)\n",
    "\n",
    "        return result.collect(engine=\"streaming\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "polars_actions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea6db30-0915-49fa-8f6d-9890ecf26114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1856bf01-ea07-46d3-843f-ed5114127471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from polars import DataFrame as PolarsDataFrame\n",
    "from pyspark.sql import functions as f, DataFrame as SparkDataFrame, SparkSession\n",
    "\n",
    "MAX_ITERATIONS = 20\n",
    "TOLERANCE = 1e-4\n",
    "CUSTOMER_COUNT = 10\n",
    "\n",
    "SPARK_OR_POLARS_DF = SparkDataFrame | PolarsDataFrame\n",
    "spark = SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bc1939-b58c-4368-97eb-b93ce3e70591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightTPCDSBase(ABC):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unique_col: str,\n",
    "            max_iterations: int = MAX_ITERATIONS,\n",
    "            tolerance: float = TOLERANCE,\n",
    "    ):\n",
    "        self.unique_col = unique_col\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        self.variables = [\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\",\n",
    "        ]\n",
    "\n",
    "        self.targets = {\n",
    "            \"cd_gender\": {\"M\": 0.48, \"F\": 0.50, \"U\": 0.02},\n",
    "            \"birth_bucket\": {\n",
    "                \"1900_1945\": 0.10,\n",
    "                \"1946_1964\": 0.30,\n",
    "                \"1965_1980\": 0.25,\n",
    "                \"1981_1996\": 0.25,\n",
    "                \"1997_plus\": 0.10,\n",
    "            },\n",
    "            \"country_bucket\": {\n",
    "                \"United States\": 0.70,\n",
    "                \"Canada\": 0.15,\n",
    "                \"United Kingdom\": 0.10,\n",
    "                \"Other\": 0.05,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def log(msg: str):\n",
    "        print(f\"[{datetime.now()}][RIM] {msg}\")\n",
    "\n",
    "    def get_target_map(self, var: str):\n",
    "        return self.targets[var]\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare(self, raw_df: SPARK_OR_POLARS_DF) -> SPARK_OR_POLARS_DF:\n",
    "        raise NotImplementedError(\"`prepare` method must be implemented in subclasses.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_ipf(self, raw_df: SPARK_OR_POLARS_DF) -> SPARK_OR_POLARS_DF:\n",
    "        raise NotImplementedError(\"`run_ipf` method must be implemented in subclasses.\")\n",
    "\n",
    "    def run(self, raw_df: SPARK_OR_POLARS_DF) -> SPARK_OR_POLARS_DF:\n",
    "        self.log(\"Старт RIM...\")\n",
    "        df_out = self.run_ipf(raw_df)\n",
    "        self.log(\"Завершено RIM.\")\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f06e56f-ea47-4cdf-b272-3ed419d3c609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfdc949-c128-487d-9bba-f2c905896f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import Broadcast\n",
    "\n",
    "\n",
    "class RimWeightSparkTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, spark: SparkSession, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "        self._spark = spark\n",
    "\n",
    "        self._spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
    "        self._spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        self._spark.conf.set(\"spark.sql.shuffle.partitions\", self._spark.sparkContext.defaultParallelism)\n",
    "\n",
    "    def prepare(self, raw_df: SparkDataFrame) -> SparkDataFrame:\n",
    "        self.log(\"Spark: bucketization + select\")\n",
    "\n",
    "        raw_df = raw_df.withColumn(\n",
    "            \"birth_bucket\",\n",
    "            f.when(f.col(\"c_birth_year\") <= 1945, \"1900_1945\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1964, \"1946_1964\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1980, \"1965_1980\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1996, \"1981_1996\")\n",
    "            .otherwise(\"1997_plus\")\n",
    "        )\n",
    "\n",
    "        raw_df = raw_df.withColumn(\n",
    "            \"country_bucket\",\n",
    "            f.when(\n",
    "                f.col(\"ca_country\").isin(\"United States\", \"Canada\", \"United Kingdom\"),\n",
    "                f.col(\"ca_country\")\n",
    "            ).otherwise(\"Other\")\n",
    "        )\n",
    "\n",
    "        return raw_df.select(\n",
    "            self.unique_col,\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\",\n",
    "            f.lit(1.0).alias(\"weight\")\n",
    "        )\n",
    "\n",
    "    def _build_broadcast_target_map(self, var: str) -> Broadcast:\n",
    "        \"\"\"Returns broadcast dict: value -> target_share\"\"\"\n",
    "\n",
    "        mp = self.get_target_map(var)\n",
    "        return self._spark.sparkContext.broadcast(mp)\n",
    "\n",
    "    def run_ipf(self, raw_df: SparkDataFrame) -> SparkDataFrame:\n",
    "        prepared_df = self.prepare(raw_df).repartition(self._spark.sparkContext.defaultParallelism)\n",
    "        total_units = prepared_df.count()\n",
    "\n",
    "        broadcast_targets = {\n",
    "            var: self._build_broadcast_target_map(var)\n",
    "            for var in self.variables\n",
    "        }\n",
    "\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Spark IPF iteration {it + 1} ---\")\n",
    "            max_diff = 0.0\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing variable: {var}\")\n",
    "\n",
    "                total_w = prepared_df.agg(f.sum(\"weight\")).first()[0]\n",
    "\n",
    "                actual = (\n",
    "                    prepared_df.groupBy(var)\n",
    "                    .agg(f.sum(\"weight\").alias(\"w_sum\"))\n",
    "                    .withColumn(\"actual_share\", f.col(\"w_sum\") / total_w)\n",
    "                )\n",
    "\n",
    "                actual_local = {\n",
    "                    r[var]: r[\"actual_share\"]\n",
    "                    for r in actual.collect()\n",
    "                }\n",
    "\n",
    "                target_map = broadcast_targets[var].value\n",
    "\n",
    "                for key, actual_val in actual_local.items():\n",
    "                    target_val = target_map.get(key, actual_val)\n",
    "                    max_diff = max(max_diff, abs(actual_val - target_val))\n",
    "\n",
    "                factor_map = {\n",
    "                    k: (target_map.get(k, v) / v) if v > 0 else 1.0\n",
    "                    for k, v in actual_local.items()\n",
    "                }\n",
    "\n",
    "                factor_expr = f.when(f.col(var).isNull(), f.lit(1.0))\n",
    "\n",
    "                for k, v in factor_map.items():\n",
    "                    factor_expr = factor_expr.when(f.col(var) == f.lit(k), f.lit(v))\n",
    "\n",
    "                factor_expr = factor_expr.otherwise(f.lit(1.0))\n",
    "\n",
    "                prepared_df = prepared_df.withColumn(\n",
    "                    \"weight\",\n",
    "                    f.col(\"weight\") * factor_expr\n",
    "                )\n",
    "                new_total = prepared_df.agg(f.sum(\"weight\")).first()[0]\n",
    "                prepared_df = prepared_df.withColumn(\"weight\", f.col(\"weight\") / new_total * total_units)\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "\n",
    "            prepared_df = prepared_df.checkpoint(eager=True)\n",
    "\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Converged.\")\n",
    "                break\n",
    "\n",
    "        return prepared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd16f987-1d3c-4f2b-9613-cc5c243de036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.table(\"samples.tpcds_sf1000.customer\").limit(CUSTOMER_COUNT)\n",
    "demo = spark.table(\"samples.tpcds_sf1000.customer_demographics\")\n",
    "addr = spark.table(\"samples.tpcds_sf1000.customer_address\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, customer.c_current_cdemo_sk == demo.cd_demo_sk)\n",
    "    .join(addr, customer.c_current_addr_sk == addr.ca_address_sk)\n",
    ")\n",
    "\n",
    "rim = RimWeightSparkTPCDS(spark)\n",
    "weighted = rim.run(df)\n",
    "\n",
    "print(weighted.count())\n",
    "display(weighted.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492d4370-0ab1-435b-b947-c29ca53d5d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Polars RIM"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark.table(\"samples.tpcds_sf1000.customer\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer\")\n",
    "spark.table(\"samples.tpcds_sf1000.customer_demographics\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer_demographics\")\n",
    "spark.table(\"samples.tpcds_sf1000.customer_address\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8c3bc0-5b2f-424a-b3e0-1a39cc3e87db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightPolarsTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "\n",
    "    def prepare(self, raw_df: PolarsDataFrame) -> PolarsDataFrame:\n",
    "        self.log(\"Polars: bucketization + select\")\n",
    "\n",
    "        needed = {\n",
    "            \"cd_gender\": None,\n",
    "            \"c_birth_year\": None,\n",
    "            \"ca_country\": None,\n",
    "        }\n",
    "\n",
    "        missing = [c for c in needed if c not in raw_df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        raw_df = raw_df.with_columns([\n",
    "            pl.when(pl.col(\"c_birth_year\") <= 1945).then(pl.lit(\"1900_1945\"))\n",
    "            .when(pl.col(\"c_birth_year\") <= 1964).then(pl.lit(\"1946_1964\"))\n",
    "            .when(pl.col(\"c_birth_year\") <= 1980).then(pl.lit(\"1965_1980\"))\n",
    "            .when(pl.col(\"c_birth_year\") <= 1996).then(pl.lit(\"1981_1996\"))\n",
    "            .otherwise(pl.lit(\"1997_plus\")).alias(\"birth_bucket\"),\n",
    "            pl.when(pl.col(\"ca_country\").is_in([\"United States\", \"Canada\", \"United Kingdom\"]))\n",
    "            .then(pl.col(\"ca_country\"))\n",
    "            .otherwise(pl.lit(\"Other\")).alias(\"country_bucket\"),\n",
    "            pl.lit(1.0).alias(\"weight\")\n",
    "        ])\n",
    "\n",
    "        return raw_df.select(\n",
    "            self.unique_col,\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\",\n",
    "            \"weight\",\n",
    "        )\n",
    "\n",
    "    def run_ipf(self, raw_df: PolarsDataFrame) -> PolarsDataFrame:\n",
    "        prepared_df = self.prepare(raw_df)\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Polars IPF iteration {it + 1} ---\")\n",
    "\n",
    "            max_diff = 0.0\n",
    "            total_w = prepared_df[\"weight\"].sum()\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing variable: {var}\")\n",
    "\n",
    "                mp = self.get_target_map(var)\n",
    "                target_df = pl.DataFrame({\n",
    "                    var: list(mp.keys()),\n",
    "                    \"target_share\": list(mp.values())\n",
    "                })\n",
    "\n",
    "                actual = (\n",
    "                    prepared_df.group_by(var)\n",
    "                    .agg(pl.sum(\"weight\").alias(\"w_sum\"))\n",
    "                    .with_columns((pl.col(\"w_sum\") / total_w).alias(\"actual_share\"))\n",
    "                )\n",
    "\n",
    "                adj = actual.join(target_df, on=var, how=\"left\").with_columns([\n",
    "                    pl.col(\"target_share\").fill_null(pl.col(\"actual_share\")),\n",
    "                    (pl.col(\"target_share\") - pl.col(\"actual_share\")).abs().alias(\"diff\")\n",
    "                ])\n",
    "\n",
    "                diff = adj[\"diff\"].max()\n",
    "                max_diff = round(max(max_diff, diff), 4)\n",
    "\n",
    "                factor_df = adj.with_columns([\n",
    "                    pl.when((pl.col(\"actual_share\") > 0) & (pl.col(\"target_share\") > 0))\n",
    "                    .then(pl.col(\"target_share\") / pl.col(\"actual_share\"))\n",
    "                    .otherwise(1.0)\n",
    "                    .alias(\"factor\")\n",
    "                ]).select(var, \"factor\")\n",
    "\n",
    "                prepared_df = prepared_df.join(factor_df, on=var, how=\"left\")\n",
    "                prepared_df = prepared_df.with_columns((pl.col(\"weight\") * pl.col(\"factor\")).alias(\"weight\")).drop(\"factor\")\n",
    "\n",
    "                new_total = prepared_df[\"weight\"].sum()\n",
    "                prepared_df = prepared_df.with_columns((pl.col(\"weight\") / new_total * prepared_df.height).alias(\"weight\"))\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Converged.\")\n",
    "                break\n",
    "\n",
    "        return prepared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a03300-0131-4141-930c-8b5f3cf3bf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer/*.parquet\").limit(CUSTOMER_COUNT)\n",
    "demo = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer_demographics/*.parquet\")\n",
    "addr = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer_address/*.parquet\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, left_on=\"c_current_cdemo_sk\", right_on=\"cd_demo_sk\", how=\"inner\")\n",
    "    .join(addr, left_on=\"c_current_addr_sk\", right_on=\"ca_address_sk\", how=\"inner\")\n",
    ")\n",
    "\n",
    "rim = RimWeightPolarsTPCDS()\n",
    "result = rim.run(df)\n",
    "\n",
    "print(result.count())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6c2b6e-ccfe-4f12-914f-c72be009206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hybrid RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "127053fe-3184-4f70-9483-3b1249df48d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightHybridTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, spark: SparkSession, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "        self._spark = spark\n",
    "\n",
    "    @staticmethod\n",
    "    def spark_to_polars(df_spark: SparkDataFrame) -> PolarsDataFrame:\n",
    "        tbl = pa.Table.from_batches(df_spark._collect_as_arrow())\n",
    "        return pl.from_arrow(tbl)\n",
    "\n",
    "    def polars_to_spark(self, df_polars: PolarsDataFrame) -> SparkDataFrame:\n",
    "        return self._spark.createDataFrame(df_polars.to_pandas())\n",
    "\n",
    "    def prepare(self, raw_df: SparkDataFrame) -> SparkDataFrame:\n",
    "        self.log(\"Hybrid: Spark bucketization\")\n",
    "\n",
    "        raw_df = raw_df.withColumn(\n",
    "            \"birth_bucket\",\n",
    "            f.when(f.col(\"c_birth_year\") <= 1945, \"1900_1945\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1964, \"1946_1964\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1980, \"1965_1980\")\n",
    "            .when(f.col(\"c_birth_year\") <= 1996, \"1981_1996\")\n",
    "            .otherwise(\"1997_plus\")\n",
    "        )\n",
    "\n",
    "        raw_df = raw_df.withColumn(\n",
    "            \"country_bucket\",\n",
    "            f.when(\n",
    "                f.col(\"ca_country\").isin(\"United States\", \"Canada\", \"United Kingdom\"),\n",
    "                f.col(\"ca_country\")\n",
    "            ).otherwise(\"Other\")\n",
    "        )\n",
    "\n",
    "        return raw_df.select(\n",
    "            self.unique_col,\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\"\n",
    "        )\n",
    "\n",
    "    def run_ipf(self, raw_df: PolarsDataFrame) -> PolarsDataFrame:\n",
    "        self.log(\"Hybrid: Polars IPF start\")\n",
    "        df_pl = raw_df.with_columns(pl.lit(1.0).alias(\"weight\"))\n",
    "\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Polars IPF iteration {it + 1} ---\")\n",
    "            max_diff = 0.0\n",
    "            total_w = df_pl[\"weight\"].sum()\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing var: {var}\")\n",
    "\n",
    "                mp = self.get_target_map(var)\n",
    "                target_df = pl.DataFrame({\n",
    "                    var: list(mp.keys()),\n",
    "                    \"target_share\": list(mp.values())\n",
    "                })\n",
    "\n",
    "                actual = (\n",
    "                    df_pl.group_by(var)\n",
    "                    .agg(pl.sum(\"weight\").alias(\"w_sum\"))\n",
    "                    .with_columns((pl.col(\"w_sum\") / total_w).alias(\"actual_share\"))\n",
    "                )\n",
    "\n",
    "                adj = actual.join(target_df, on=var, how=\"left\").with_columns([\n",
    "                    pl.col(\"target_share\").fill_null(pl.col(\"actual_share\")),\n",
    "                    (pl.col(\"target_share\") - pl.col(\"actual_share\")).abs().alias(\"diff\")\n",
    "                ])\n",
    "\n",
    "                diff = adj[\"diff\"].max()\n",
    "                max_diff = max(max_diff, diff)\n",
    "\n",
    "                factor_df = adj.with_columns([\n",
    "                    pl.when((pl.col(\"actual_share\") > 0) & (pl.col(\"target_share\") > 0))\n",
    "                    .then(pl.col(\"target_share\") / pl.col(\"actual_share\"))\n",
    "                    .otherwise(1.0)\n",
    "                    .alias(\"factor\")\n",
    "                ]).select(var, \"factor\")\n",
    "\n",
    "                df_pl = df_pl.join(factor_df, on=var, how=\"left\")\n",
    "                df_pl = df_pl.with_columns((pl.col(\"weight\") * pl.col(\"factor\")).alias(\"weight\"))\n",
    "                df_pl = df_pl.drop(\"factor\")\n",
    "\n",
    "                new_total = df_pl[\"weight\"].sum()\n",
    "                df_pl = df_pl.with_columns((pl.col(\"weight\") / new_total * df_pl.height).alias(\"weight\"))\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Polars converged.\")\n",
    "                break\n",
    "\n",
    "        return df_pl\n",
    "\n",
    "    def run(self, df_spark: SparkDataFrame) -> SparkDataFrame:\n",
    "        df_prep = self.prepare(df_spark)\n",
    "        df_pl = self.spark_to_polars(df_prep)\n",
    "        df_out = self.run(df_pl)\n",
    "        return self.polars_to_spark(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977272a0-1d79-44b8-bb52-03c9a8a176a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.table(\"samples.tpcds_sf1000.customer\").limit(CUSTOMER_COUNT)\n",
    "demo = spark.table(\"samples.tpcds_sf1000.customer_demographics\")\n",
    "addr = spark.table(\"samples.tpcds_sf1000.customer_address\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, customer.c_current_cdemo_sk == demo.cd_demo_sk)\n",
    "    .join(addr, customer.c_current_addr_sk == addr.ca_address_sk)\n",
    ")\n",
    "\n",
    "rim = RimWeightHybridTPCDS(spark)\n",
    "result = rim.run(df)\n",
    "print(result.count())\n",
    "display(result.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rim",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

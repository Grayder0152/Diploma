{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea6db30-0915-49fa-8f6d-9890ecf26114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1856bf01-ea07-46d3-843f-ed5114127471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "MAX_ITERATIONS = 20\n",
    "TOLERANCE = 1e-4\n",
    "CUSTOMER_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8186dfae-f565-4697-a984-abc24e1c84b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"samples.tpcds_sf1000.customer\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer\")\n",
    "spark.table(\"samples.tpcds_sf1000.customer_demographics\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer_demographics\")\n",
    "spark.table(\"samples.tpcds_sf1000.customer_address\").write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/tpcds_sf1000/customer_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bc1939-b58c-4368-97eb-b93ce3e70591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightTPCDSBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_col: str,\n",
    "        max_iterations: int = MAX_ITERATIONS,\n",
    "        tolerance: float = TOLERANCE,\n",
    "    ):\n",
    "        self.unique_col = unique_col\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        self.variables = [\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\",\n",
    "        ]\n",
    "\n",
    "        self.targets = {\n",
    "            \"cd_gender\": {\"M\": 0.48, \"F\": 0.50, \"U\": 0.02},\n",
    "            # \"cd_credit_rating\": {\n",
    "            #     \"Excellent\": 0.25,\n",
    "            #     \"Good\": 0.35,\n",
    "            #     \"Fair\": 0.25,\n",
    "            #     \"Poor\": 0.15,\n",
    "            # },\n",
    "            \"birth_bucket\": {\n",
    "                \"1900_1945\": 0.10,\n",
    "                \"1946_1964\": 0.30,\n",
    "                \"1965_1980\": 0.25,\n",
    "                \"1981_1996\": 0.25,\n",
    "                \"1997_plus\": 0.10,\n",
    "            },\n",
    "            \"country_bucket\": {\n",
    "                \"United States\": 0.70,\n",
    "                \"Canada\": 0.15,\n",
    "                \"United Kingdom\": 0.10,\n",
    "                \"Other\": 0.05,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def log(self, msg: str):\n",
    "        print(f\"[{datetime.now()}][RIM] {msg}\")\n",
    "\n",
    "    def get_target_map(self, var: str):\n",
    "        return self.targets[var]\n",
    "\n",
    "    def run_ipf(self, df):\n",
    "        raise NotImplementedError(\"run_ipf must be implemented in subclasses.\")\n",
    "\n",
    "    def run(self, df):\n",
    "        self.log(\"Старт RIM...\")\n",
    "        df_out = self.run_ipf(df)\n",
    "        self.log(\"Завершено RIM.\")\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f06e56f-ea47-4cdf-b272-3ed419d3c609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfdc949-c128-487d-9bba-f2c905896f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightSparkTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "        spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
    "\n",
    "    def prepare(self, df: DataFrame) -> DataFrame:\n",
    "        self.log(\"Spark: bucketization + select\")\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"birth_bucket\",\n",
    "            F.when(F.col(\"c_birth_year\") <= 1945, \"1900_1945\")\n",
    "            .when(F.col(\"c_birth_year\") <= 1964, \"1946_1964\")\n",
    "            .when(F.col(\"c_birth_year\") <= 1980, \"1965_1980\")\n",
    "            .when(F.col(\"c_birth_year\") <= 1996, \"1981_1996\")\n",
    "            .otherwise(\"1997_plus\")\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"country_bucket\",\n",
    "            F.when(\n",
    "                F.col(\"ca_country\").isin(\"United States\", \"Canada\", \"United Kingdom\"),\n",
    "                F.col(\"ca_country\")\n",
    "            ).otherwise(\"Other\")\n",
    "        )\n",
    "\n",
    "        df = df.select(\n",
    "            self.unique_col,\n",
    "            \"cd_gender\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\"\n",
    "        )\n",
    "\n",
    "        return df.withColumn(\"weight\", F.lit(1.0))\n",
    "\n",
    "    def build_broadcast_target_map(self, spark, var):\n",
    "        \"\"\"Returns broadcast dict: value -> target_share\"\"\"\n",
    "        mp = self.get_target_map(var)\n",
    "        return spark.sparkContext.broadcast(mp)\n",
    "\n",
    "    def run_ipf(self, df: DataFrame):\n",
    "        spark = df.sql_ctx.sparkSession\n",
    "\n",
    "        spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "        df = self.prepare(df)\n",
    "        df = df.repartition(spark.sparkContext.defaultParallelism)\n",
    "\n",
    "        total_units = df.count()\n",
    "\n",
    "        broadcast_targets = {\n",
    "            var: self.build_broadcast_target_map(spark, var)\n",
    "            for var in self.variables\n",
    "        }\n",
    "\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Spark IPF iteration {it+1} ---\")\n",
    "            max_diff = 0.0\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing variable: {var}\")\n",
    "\n",
    "                total_w = df.agg(F.sum(\"weight\")).first()[0]\n",
    "\n",
    "                actual = (\n",
    "                    df.groupBy(var)\n",
    "                    .agg(F.sum(\"weight\").alias(\"w_sum\"))\n",
    "                    .withColumn(\"actual_share\", F.col(\"w_sum\") / total_w)\n",
    "                )\n",
    "\n",
    "                actual_local = {\n",
    "                    r[var]: r[\"actual_share\"]\n",
    "                    for r in actual.collect()\n",
    "                }\n",
    "\n",
    "                target_map = broadcast_targets[var].value\n",
    "\n",
    "                for key, actual_val in actual_local.items():\n",
    "                    target_val = target_map.get(key, actual_val)\n",
    "                    max_diff = max(max_diff, abs(actual_val - target_val))\n",
    "\n",
    "                factor_map = {\n",
    "                    k: (target_map.get(k, v) / v) if v > 0 else 1.0\n",
    "                    for k, v in actual_local.items()\n",
    "                }\n",
    "\n",
    "                factor_expr = F.when(F.col(var).isNull(), F.lit(1.0))\n",
    "\n",
    "                for k, v in factor_map.items():\n",
    "                    factor_expr = factor_expr.when(F.col(var) == F.lit(k), F.lit(v))\n",
    "\n",
    "                factor_expr = factor_expr.otherwise(F.lit(1.0))\n",
    "\n",
    "                df = df.withColumn(\n",
    "                    \"weight\",\n",
    "                    F.col(\"weight\") * factor_expr\n",
    "                )\n",
    "                new_total = df.agg(F.sum(\"weight\")).first()[0]\n",
    "                df = df.withColumn(\"weight\", F.col(\"weight\") / new_total * total_units)\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "\n",
    "            df = df.checkpoint(eager=True)\n",
    "\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Converged.\")\n",
    "                break\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd16f987-1d3c-4f2b-9613-cc5c243de036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.table(\"samples.tpcds_sf1000.customer\").limit(11579572)\n",
    "demo = spark.table(\"samples.tpcds_sf1000.customer_demographics\")\n",
    "addr = spark.table(\"samples.tpcds_sf1000.customer_address\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, customer.c_current_cdemo_sk == demo.cd_demo_sk)\n",
    "    .join(addr, customer.c_current_addr_sk == addr.ca_address_sk)\n",
    ")\n",
    "\n",
    "rim = RimWeightSparkTPCDS()\n",
    "weighted = rim.run(df)\n",
    "\n",
    "print(weighted.count())\n",
    "display(weighted.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492d4370-0ab1-435b-b947-c29ca53d5d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Polars RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8c3bc0-5b2f-424a-b3e0-1a39cc3e87db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightPolarsTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "\n",
    "    def prepare(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "            self.log(\"Polars: bucketization + select\")\n",
    "\n",
    "            needed = {\n",
    "                \"cd_gender\": None,\n",
    "                # \"cd_credit_rating\": None,\n",
    "                \"c_birth_year\": None,\n",
    "                \"ca_country\": None,\n",
    "            }\n",
    "\n",
    "            missing = [c for c in needed if c not in df.columns]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "            df = df.with_columns([\n",
    "                pl.when(pl.col(\"c_birth_year\") <= 1945).then(pl.lit(\"1900_1945\"))\n",
    "                .when(pl.col(\"c_birth_year\") <= 1964).then(pl.lit(\"1946_1964\"))\n",
    "                .when(pl.col(\"c_birth_year\") <= 1980).then(pl.lit(\"1965_1980\"))\n",
    "                .when(pl.col(\"c_birth_year\") <= 1996).then(pl.lit(\"1981_1996\"))\n",
    "                .otherwise(pl.lit(\"1997_plus\")).alias(\"birth_bucket\"),\n",
    "\n",
    "                pl.when(pl.col(\"ca_country\").is_in([\"United States\",\"Canada\",\"United Kingdom\"]))\n",
    "                .then(pl.col(\"ca_country\"))\n",
    "                .otherwise(pl.lit(\"Other\")).alias(\"country_bucket\"),\n",
    "\n",
    "                pl.lit(1.0).alias(\"weight\")\n",
    "            ])\n",
    "\n",
    "            return df.select(\n",
    "                self.unique_col,\n",
    "                \"cd_gender\",\n",
    "                # \"cd_credit_rating\",\n",
    "                \"birth_bucket\",\n",
    "                \"country_bucket\",\n",
    "                \"weight\",\n",
    "            )\n",
    "\n",
    "    def run_ipf(self, df: pl.DataFrame):\n",
    "        df = self.prepare(df)\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Polars IPF iteration {it+1} ---\")\n",
    "\n",
    "            max_diff = 0.0\n",
    "            total_w = df[\"weight\"].sum()\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing variable: {var}\")\n",
    "\n",
    "                mp = self.get_target_map(var)\n",
    "                target_df = pl.DataFrame({\n",
    "                    var: list(mp.keys()),\n",
    "                    \"target_share\": list(mp.values())\n",
    "                })\n",
    "\n",
    "                actual = (\n",
    "                    df.group_by(var)\n",
    "                      .agg(pl.sum(\"weight\").alias(\"w_sum\"))\n",
    "                      .with_columns((pl.col(\"w_sum\")/total_w).alias(\"actual_share\"))\n",
    "                )\n",
    "\n",
    "                adj = actual.join(target_df, on=var, how=\"left\").with_columns([\n",
    "                    pl.col(\"target_share\").fill_null(pl.col(\"actual_share\")),\n",
    "                    (pl.col(\"target_share\") - pl.col(\"actual_share\")).abs().alias(\"diff\")\n",
    "                ])\n",
    "\n",
    "                diff = adj[\"diff\"].max()\n",
    "                max_diff = round(max(max_diff, diff), 4)\n",
    "\n",
    "                factor_df = adj.with_columns([\n",
    "                    pl.when((pl.col(\"actual_share\")>0)&(pl.col(\"target_share\")>0))\n",
    "                      .then(pl.col(\"target_share\")/pl.col(\"actual_share\"))\n",
    "                      .otherwise(1.0)\n",
    "                      .alias(\"factor\")\n",
    "                ]).select(var,\"factor\")\n",
    "\n",
    "                df = df.join(factor_df, on=var, how=\"left\")\n",
    "                df = df.with_columns((pl.col(\"weight\")*pl.col(\"factor\")).alias(\"weight\"))\n",
    "                df = df.drop(\"factor\")\n",
    "\n",
    "                new_total = df[\"weight\"].sum()\n",
    "                df = df.with_columns((pl.col(\"weight\")/new_total * df.height).alias(\"weight\"))\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Converged.\")\n",
    "                break\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a03300-0131-4141-930c-8b5f3cf3bf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer/*.parquet\").limit(11579572)\n",
    "demo = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer_demographics/*.parquet\")\n",
    "addr = pl.read_parquet(\"/dbfs/tmp/tpcds_sf1000/customer_address/*.parquet\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, left_on=\"c_current_cdemo_sk\", right_on=\"cd_demo_sk\", how=\"inner\")\n",
    "    .join(addr, left_on=\"c_current_addr_sk\", right_on=\"ca_address_sk\", how=\"inner\")\n",
    ")\n",
    "\n",
    "rim = RimWeightPolarsTPCDS()\n",
    "result = rim.run(df)\n",
    "\n",
    "print(result.count())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6c2b6e-ccfe-4f12-914f-c72be009206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hybrid RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "127053fe-3184-4f70-9483-3b1249df48d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RimWeightHybridTPCDS(RimWeightTPCDSBase):\n",
    "    def __init__(self, unique_col=\"c_customer_sk\", **kwargs):\n",
    "        super().__init__(unique_col, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def spark_to_polars(df_spark: DataFrame) -> pl.DataFrame:\n",
    "        tbl = pa.Table.from_batches(df_spark._collect_as_arrow())\n",
    "        return pl.from_arrow(tbl)\n",
    "\n",
    "    @staticmethod\n",
    "    def polars_to_spark(spark, df_polars: pl.DataFrame) -> DataFrame:\n",
    "        return spark.createDataFrame(df_polars.to_pandas())\n",
    "\n",
    "    def prepare_spark(self, df: DataFrame) -> DataFrame:\n",
    "        self.log(\"Hybrid: Spark bucketization\")\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"birth_bucket\",\n",
    "            F.when(F.col(\"c_birth_year\")<=1945,\"1900_1945\")\n",
    "             .when(F.col(\"c_birth_year\")<=1964,\"1946_1964\")\n",
    "             .when(F.col(\"c_birth_year\")<=1980,\"1965_1980\")\n",
    "             .when(F.col(\"c_birth_year\")<=1996,\"1981_1996\")\n",
    "             .otherwise(\"1997_plus\")\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"country_bucket\",\n",
    "            F.when(\n",
    "                F.col(\"ca_country\").isin(\"United States\",\"Canada\",\"United Kingdom\"),\n",
    "                F.col(\"ca_country\")\n",
    "            ).otherwise(\"Other\")\n",
    "        )\n",
    "\n",
    "        df = df.select(\n",
    "            self.unique_col,\n",
    "            \"cd_gender\",\n",
    "            # \"cd_credit_rating\",\n",
    "            \"birth_bucket\",\n",
    "            \"country_bucket\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def run_ipf_polars(self, df_pl: pl.DataFrame) -> pl.DataFrame:\n",
    "        self.log(\"Hybrid: Polars IPF start\")\n",
    "        df_pl = df_pl.with_columns(pl.lit(1.0).alias(\"weight\"))\n",
    "\n",
    "        for it in range(self.max_iterations):\n",
    "            self.log(f\"--- Polars IPF iteration {it+1} ---\")\n",
    "            max_diff = 0.0\n",
    "            total_w = df_pl[\"weight\"].sum()\n",
    "\n",
    "            for var in self.variables:\n",
    "                self.log(f\"processing var: {var}\")\n",
    "\n",
    "                mp = self.get_target_map(var)\n",
    "                target_df = pl.DataFrame({\n",
    "                    var: list(mp.keys()),\n",
    "                    \"target_share\": list(mp.values())\n",
    "                })\n",
    "\n",
    "                actual = (\n",
    "                    df_pl.group_by(var)\n",
    "                        .agg(pl.sum(\"weight\").alias(\"w_sum\"))\n",
    "                        .with_columns((pl.col(\"w_sum\")/total_w).alias(\"actual_share\"))\n",
    "                )\n",
    "\n",
    "                adj = actual.join(target_df, on=var, how=\"left\").with_columns([\n",
    "                    pl.col(\"target_share\").fill_null(pl.col(\"actual_share\")),\n",
    "                    (pl.col(\"target_share\")-pl.col(\"actual_share\")).abs().alias(\"diff\")\n",
    "                ])\n",
    "\n",
    "                diff = adj[\"diff\"].max()\n",
    "                max_diff = max(max_diff, diff)\n",
    "\n",
    "                factor_df = adj.with_columns([\n",
    "                    pl.when((pl.col(\"actual_share\")>0)&(pl.col(\"target_share\")>0))\n",
    "                      .then(pl.col(\"target_share\")/pl.col(\"actual_share\"))\n",
    "                      .otherwise(1.0)\n",
    "                      .alias(\"factor\")\n",
    "                ]).select(var,\"factor\")\n",
    "\n",
    "                df_pl = df_pl.join(factor_df, on=var, how=\"left\")\n",
    "                df_pl = df_pl.with_columns((pl.col(\"weight\")*pl.col(\"factor\")).alias(\"weight\"))\n",
    "                df_pl = df_pl.drop(\"factor\")\n",
    "\n",
    "                new_total = df_pl[\"weight\"].sum()\n",
    "                df_pl = df_pl.with_columns((pl.col(\"weight\")/new_total * df_pl.height).alias(\"weight\"))\n",
    "\n",
    "            self.log(f\"iteration max_diff={max_diff}\")\n",
    "            if max_diff <= self.tolerance:\n",
    "                self.log(\"Polars converged.\")\n",
    "                break\n",
    "\n",
    "        return df_pl\n",
    "\n",
    "    def run_ipf(self, df_spark: DataFrame):\n",
    "        spark = df_spark.sql_ctx.sparkSession\n",
    "\n",
    "        df_prep = self.prepare_spark(df_spark)\n",
    "        df_pl = self.spark_to_polars(df_prep)\n",
    "\n",
    "        df_pl_weighted = self.run_ipf_polars(df_pl)\n",
    "\n",
    "        df_final = self.polars_to_spark(spark, df_pl_weighted)\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977272a0-1d79-44b8-bb52-03c9a8a176a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer = spark.table(\"samples.tpcds_sf1000.customer\").limit(11579572)\n",
    "demo = spark.table(\"samples.tpcds_sf1000.customer_demographics\")\n",
    "addr = spark.table(\"samples.tpcds_sf1000.customer_address\")\n",
    "\n",
    "df = (\n",
    "    customer\n",
    "    .join(demo, customer.c_current_cdemo_sk == demo.cd_demo_sk)\n",
    "    .join(addr, customer.c_current_addr_sk == addr.ca_address_sk)\n",
    ")\n",
    "\n",
    "rim = RimWeightHybridTPCDS()\n",
    "result = rim.run(df)\n",
    "print(result.count())\n",
    "display(result.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rim",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
